{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4956c55",
   "metadata": {},
   "source": [
    "# YCITE: Model with Explanation\n",
    "\n",
    "This notebook consist of the code that are used to evaluate the final result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5efec7e",
   "metadata": {},
   "source": [
    "### The next section runs the model (with LIME), produce the outcome for the dataset and determine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f86a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual experiment that find the cosine similarity score for everything\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import signal\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Timeout handler for long-running LIME explanations\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException\n",
    "\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "# Load SpaCy's English model and stop words\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "punctuation_table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "# Initialize SentenceTransformer for BERT-based similarity\n",
    "bert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.load_state_dict(torch.load(\"./scibert_fine_tuned_model_LIME.pth\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir='./local_model')\n",
    "\n",
    "# Define prediction function for LIME\n",
    "def predict_proba(texts):\n",
    "    tokens = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# Initialize LIME explainer\n",
    "explainer = LimeTextExplainer(class_names=['No Citation Needed', 'Citation Needed'])\n",
    "\n",
    "# Helper function to clean and tokenize text\n",
    "def clean_and_tokenize(text):\n",
    "    text = text.translate(punctuation_table).lower()  # Remove punctuation and lowercase\n",
    "    tokens = [token.text for token in nlp(text) if token.text not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Generate explanations with cleaned multi-word phrases\n",
    "def generate_explanation(lime_keywords_pos, sentence, label):\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Extract multi-word phrases\n",
    "    multi_word_phrases = [\n",
    "        chunk.text for chunk in doc.noun_chunks\n",
    "    ] + [\n",
    "        entity.text for entity in doc.ents\n",
    "    ] + [\n",
    "        f\"{token.head.text} {token.text}\" for token in doc if token.dep_ == \"amod\"\n",
    "    ]\n",
    "    \n",
    "    # Remove duplicates, normalize, and filter out empty strings\n",
    "    multi_word_phrases = list(set([\n",
    "        ' '.join(sorted([word for word in phrase.lower().split() if word not in stop_words])).strip()\n",
    "        for phrase in multi_word_phrases\n",
    "    ]))\n",
    "    multi_word_phrases = [phrase for phrase in multi_word_phrases if phrase]  # Remove empty strings\n",
    "\n",
    "    # Extract relationships\n",
    "    relationships = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in {\"nsubj\", \"nsubjpass\"} and token.head.pos_ == \"VERB\":\n",
    "            objs = [obj.text for obj in token.head.children if obj.dep_ in {\"dobj\", \"pobj\", \"attr\"}]\n",
    "            relationships.append((token.text, token.head.text, objs))\n",
    "\n",
    "    explanation = []\n",
    "    global used_phrases\n",
    "    used_phrases = []  # Track used phrases throughout the explanation generation process\n",
    "\n",
    "    if label == 1:  # \"Citation Needed\" template\n",
    "        for subj, verb, objs in relationships:\n",
    "            if objs:\n",
    "                obj_list = \", \".join(objs)\n",
    "                standardized_subj = ' '.join(sorted(subj.lower().split()))\n",
    "                if standardized_subj not in used_phrases:\n",
    "                    explanation_text = f\"The effect of action '{verb.upper()}' on '{subj}' and '{obj_list}' requires evidence to support the claim.\"\n",
    "                    explanation.append(explanation_text)\n",
    "                    used_phrases.append(standardized_subj)\n",
    "                    print(f\"Debug: Adding relationship explanation - subj: '{subj}', verb: '{verb}', objs: '{obj_list}'\")\n",
    "        for phrase in multi_word_phrases[:3]:  # Limit to top 3 phrases to reduce redundancy\n",
    "            standardized_phrase = ' '.join(sorted(phrase.lower().split()))\n",
    "            if standardized_phrase not in used_phrases:\n",
    "                explanation_text = f\"The phrase '{phrase}' presents a claim that requires substantiation.\"\n",
    "                explanation.append(explanation_text)\n",
    "                used_phrases.append(standardized_phrase)\n",
    "                print(f\"Debug: Adding multi-word phrase explanation - phrase: '{phrase}'\")\n",
    "\n",
    "    else:  # \"No Citation Needed\" template\n",
    "        if \"result\" in sentence.lower() or \"method\" in sentence.lower():\n",
    "            if \"result\" not in used_phrases:\n",
    "                explanation_text = \"This sentence reports study-specific findings, not requiring citation.\"\n",
    "                explanation.append(explanation_text)\n",
    "                used_phrases.append(\"result\")\n",
    "                print(f\"Debug: Adding 'No Citation Needed' explanation - {explanation_text}\")\n",
    "        elif any(token.dep_ == \"ROOT\" and token.pos_ == \"AUX\" for token in doc):\n",
    "            if \"logical deduction\" not in used_phrases:\n",
    "                explanation_text = \"This sentence reflects logical deduction and doesn't need citation.\"\n",
    "                explanation.append(explanation_text)\n",
    "                used_phrases.append(\"logical deduction\")\n",
    "                print(f\"Debug: Adding logical deduction explanation - {explanation_text}\")\n",
    "        elif any(token.ent_type_ in {\"DATE\", \"TIME\", \"PERCENT\", \"QUANTITY\"} for token in doc):\n",
    "            if \"quantitative information\" not in used_phrases:\n",
    "                explanation_text = \"This sentence provides quantitative information that is self-contained.\"\n",
    "                explanation.append(explanation_text)\n",
    "                used_phrases.append(\"quantitative information\")\n",
    "                print(f\"Debug: Adding quantitative information explanation - {explanation_text}\")\n",
    "        elif any(token.text.lower() in {\"this\", \"these\", \"our\"} for token in doc):\n",
    "            if \"specific study findings\" not in used_phrases:\n",
    "                explanation_text = \"This sentence refers to specific study findings, which are sufficiently explained and don't need external citation.\"\n",
    "                explanation.append(explanation_text)\n",
    "                used_phrases.append(\"specific study findings\")\n",
    "                print(f\"Debug: Adding specific study findings explanation - {explanation_text}\")\n",
    "        else:\n",
    "            if \"general background information\" not in used_phrases:\n",
    "                explanation_text = \"This sentence provides general background information, making external citation unnecessary in this context.\"\n",
    "                explanation.append(explanation_text)\n",
    "                used_phrases.append(\"general background information\")\n",
    "                print(f\"Debug: Adding general background information explanation - {explanation_text}\")\n",
    "\n",
    "    if not explanation:\n",
    "        explanation.append(\"This sentence provides background or context without requiring citation.\")\n",
    "        print(f\"Debug: Adding default explanation - This sentence provides background or context without requiring citation.\")\n",
    "    explanation_tokens = [\n",
    "        token.lower() for phrase in used_phrases for token in phrase.split()\n",
    "    ]\n",
    "    return \" \".join(explanation), explanation_tokens\n",
    "\n",
    "\n",
    "# Calculate BERT Cosine Similarity\n",
    "def calculate_bert_cosine_similarity(text1, text2):\n",
    "    # Encode the texts\n",
    "    embeddings1 = bert_model.encode(text1, convert_to_tensor=True)\n",
    "    embeddings2 = bert_model.encode(text2, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity_score = util.pytorch_cos_sim(embeddings1, embeddings2).item()\n",
    "    return similarity_score\n",
    "\n",
    "\n",
    "# Token-level comparison with intersection of original sentence, LLM explanation, and filtered LIME tokens\n",
    "def token_level_comparison_with_intersection(sentence_tokens, llm_tokens, lime_tokens, used_phrases):\n",
    "    # Filter LIME tokens to include only those in used_phrases\n",
    "    filtered_lime_tokens = [\n",
    "        token.lower() for phrase in used_phrases for token in phrase.split()\n",
    "    ]\n",
    "\n",
    "    # Create the intersection of original sentence and LLM explanation tokens\n",
    "    intersection_tokens = set(sentence_tokens).intersection(set(llm_tokens))\n",
    "\n",
    "    # Identify matched tokens between the intersection and the filtered LIME-derived tokens\n",
    "    matched_tokens = [token for token in filtered_lime_tokens if token in intersection_tokens]\n",
    "\n",
    "    return matched_tokens, intersection_tokens, filtered_lime_tokens\n",
    "\n",
    "# Summarize results for final analysis with total and micro/macro averages\n",
    "def summarize_results(df, results, global_metrics):\n",
    "    summary = {\n",
    "        \"bert_similarity\": {\n",
    "            \"all_sentences\": [],\n",
    "            \"category_2_and_3\": [],\n",
    "            \"category_3\": []\n",
    "        },\n",
    "        \"token_metrics\": {\n",
    "            \"precision\": [],\n",
    "            \"recall\": [],\n",
    "            \"f1\": []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for res in results:\n",
    "        summary[\"bert_similarity\"][\"all_sentences\"].append(res[\"bert_similarity\"])\n",
    "        if res[\"category\"] in [2, 3]:\n",
    "            summary[\"bert_similarity\"][\"category_2_and_3\"].append(res[\"bert_similarity\"])\n",
    "        if res[\"category\"] == 3:\n",
    "            summary[\"bert_similarity\"][\"category_3\"].append(res[\"bert_similarity\"])\n",
    "        \n",
    "        summary[\"token_metrics\"][\"precision\"].append(res[\"precision\"])\n",
    "        summary[\"token_metrics\"][\"recall\"].append(res[\"recall\"])\n",
    "        summary[\"token_metrics\"][\"f1\"].append(res[\"f1\"])\n",
    "    \n",
    "    # Handle empty lists to avoid ZeroDivisionError\n",
    "    def safe_average(lst):\n",
    "        return sum(lst) / len(lst) if len(lst) > 0 else 0\n",
    "\n",
    "    final_summary = {\n",
    "        \"average_bert_similarity\": {\n",
    "            \"all_sentences\": safe_average(summary[\"bert_similarity\"][\"all_sentences\"]),\n",
    "            \"category_2_and_3\": safe_average(summary[\"bert_similarity\"][\"category_2_and_3\"]),\n",
    "            \"category_3\": safe_average(summary[\"bert_similarity\"][\"category_3\"]),\n",
    "        },\n",
    "        \"average_token_metrics\": {\n",
    "            \"precision\": safe_average(summary[\"token_metrics\"][\"precision\"]),\n",
    "            \"recall\": safe_average(summary[\"token_metrics\"][\"recall\"]),\n",
    "            \"f1\": safe_average(summary[\"token_metrics\"][\"f1\"]),\n",
    "        },\n",
    "        \"total_metrics\": global_metrics  # Add total precision, recall, and F1 from global metrics\n",
    "    }\n",
    "    \n",
    "    return final_summary\n",
    "\n",
    "\n",
    "# Categorize and log sentences\n",
    "def categorize_and_log(df, lime_timeout=10):\n",
    "    results = []  # Store per-sentence metrics for final analysis\n",
    "    citation_needed_category_3_results = []  # For \"Citation Needed\" in category 3\n",
    "    global_tp, global_fp, global_fn = 0, 0, 0  # For total precision/recall calculations\n",
    "\n",
    "#     if max_rows is not None:\n",
    "#         df = df.head(max_rows)\n",
    "\n",
    "    with open(\"category_1.log\", \"w\") as log1, open(\"category_2.log\", \"w\") as log2, open(\"category_3.log\", \"w\") as log3:\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing sentences\"):\n",
    "            sentence = row['current_sentence']\n",
    "            llm_explanation = row['explanation']\n",
    "            expected_label = row['label']\n",
    "\n",
    "            # Predict label and generate LIME explanation\n",
    "            predicted_probs = predict_proba([sentence])[0]\n",
    "            predicted_label = int(predicted_probs.argmax())\n",
    "\n",
    "            signal.alarm(lime_timeout)\n",
    "            try:\n",
    "                explanation = explainer.explain_instance(\n",
    "                    sentence,\n",
    "                    predict_proba,\n",
    "                    num_features=10,\n",
    "                    num_samples=200\n",
    "                )\n",
    "                signal.alarm(0)\n",
    "\n",
    "                lime_results = explanation.as_list()\n",
    "                lime_tokens = [word.lower() for word, _ in lime_results]  # LIME-derived tokens\n",
    "\n",
    "                # Generate explanation and extract tokens from used phrases\n",
    "                generated_explanation, explanation_tokens = generate_explanation(\n",
    "                    lime_results, sentence, predicted_label\n",
    "                )\n",
    "\n",
    "                # Token-level comparison\n",
    "                sentence_tokens = clean_and_tokenize(sentence)\n",
    "                llm_tokens = clean_and_tokenize(llm_explanation)\n",
    "\n",
    "                # Pass explanation tokens and used phrases\n",
    "                matched_tokens, intersection_tokens, filtered_lime_tokens = token_level_comparison_with_intersection(\n",
    "                    sentence_tokens, llm_tokens, explanation_tokens, used_phrases\n",
    "                )\n",
    "\n",
    "                # Calculate precision, recall, and F1 using explanation tokens\n",
    "                tp = len(matched_tokens)\n",
    "                fp = len(explanation_tokens) - tp\n",
    "                fn = len(intersection_tokens) - tp\n",
    "                precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "                recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "                f1 = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "\n",
    "                # Update global metrics\n",
    "                global_tp += tp\n",
    "                global_fp += fp\n",
    "                global_fn += fn\n",
    "\n",
    "                # Log metrics for each sentence\n",
    "                if predicted_label != expected_label:\n",
    "                    category = 1\n",
    "                    log_file = log1\n",
    "                elif not intersection_tokens:\n",
    "                    category = 2\n",
    "                    log_file = log2\n",
    "                else:\n",
    "                    category = 3\n",
    "                    log_file = log3\n",
    "\n",
    "                # Log metrics for each sentence\n",
    "                log_file.write(\"\\n=== Debug Info ===\\n\")\n",
    "                log_file.write(f\"Sentence: {sentence}\\n\")\n",
    "                log_file.write(f\"Expected Label: {'Citation Needed' if expected_label == 1 else 'No Citation Needed'}\\n\")\n",
    "                log_file.write(f\"Predicted Label: {'Citation Needed' if predicted_label == 1 else 'No Citation Needed'}\\n\")\n",
    "                log_file.write(f\"Generated Explanation: {generated_explanation}\\n\")\n",
    "                log_file.write(f\"LLM Explanation: {llm_explanation}\\n\")\n",
    "                log_file.write(f\"Original Sentence Tokens: {sentence_tokens}\\n\")\n",
    "                log_file.write(f\"LLM Explanation Tokens: {llm_tokens}\\n\")\n",
    "                log_file.write(f\"Filtered LIME Tokens (from used phrases): {filtered_lime_tokens}\\n\")\n",
    "                log_file.write(f\"Intersection Tokens: {intersection_tokens}\\n\")\n",
    "                log_file.write(f\"Matched Tokens: {matched_tokens}\\n\")\n",
    "\n",
    "                # Add TP, FP, FN to the log\n",
    "                log_file.write(f\"True Positives (TP): {tp}\\n\")\n",
    "                log_file.write(f\"False Positives (FP): {fp}\\n\")\n",
    "                log_file.write(f\"False Negatives (FN): {fn}\\n\")\n",
    "\n",
    "                log_file.write(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\\n\")\n",
    "                log_file.write(\"===================\\n\\n\")\n",
    "\n",
    "                results.append({\n",
    "                    \"category\": category,\n",
    "                    \"bert_similarity\": calculate_bert_cosine_similarity(llm_explanation, generated_explanation),\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"f1\": f1\n",
    "                })\n",
    "\n",
    "                if category == 3 and expected_label == 1:\n",
    "                    citation_needed_category_3_results.append({\n",
    "                        \"category\": category,\n",
    "                        \"bert_similarity\": calculate_bert_cosine_similarity(llm_explanation, generated_explanation),\n",
    "                        \"precision\": precision,\n",
    "                        \"recall\": recall,\n",
    "                        \"f1\": f1\n",
    "                    })\n",
    "\n",
    "            except TimeoutException:\n",
    "                log1.write(f\"Skipped sentence due to timeout: {sentence}\\n\")\n",
    "                signal.alarm(0)\n",
    "                continue\n",
    "\n",
    "    # Calculate global metrics for total precision/recall\n",
    "    total_precision = global_tp / (global_tp + global_fp) if (global_tp + global_fp) > 0 else 0\n",
    "    total_recall = global_tp / (global_tp + global_fn) if (global_tp + global_fn) > 0 else 0\n",
    "    total_f1 = (2 * total_precision * total_recall) / (total_precision + total_recall) if total_precision + total_recall > 0 else 0\n",
    "\n",
    "    global_metrics = {\n",
    "        \"total_precision\": total_precision,\n",
    "        \"total_recall\": total_recall,\n",
    "        \"total_f1\": total_f1\n",
    "    }\n",
    "\n",
    "    # Summarize and log final analysis\n",
    "    summary = summarize_results(df, results, global_metrics)\n",
    "    citation_needed_category_3_summary = summarize_results(df, citation_needed_category_3_results, global_metrics)\n",
    "\n",
    "    with open(\"final_analysis.log\", \"w\") as final_log:\n",
    "        final_log.write(\"=== Final Analysis ===\\n\")\n",
    "        final_log.write(f\"Average BERT Similarity (All Sentences): {summary['average_bert_similarity']['all_sentences']:.4f}\\n\")\n",
    "        final_log.write(f\"Average BERT Similarity (Category 2 & 3): {summary['average_bert_similarity']['category_2_and_3']:.4f}\\n\")\n",
    "        final_log.write(f\"Average BERT Similarity (Category 3): {summary['average_bert_similarity']['category_3']:.4f}\\n\")\n",
    "        final_log.write(f\"Average Precision: {summary['average_token_metrics']['precision']:.4f}\\n\")\n",
    "        final_log.write(f\"Average Recall: {summary['average_token_metrics']['recall']:.4f}\\n\")\n",
    "        final_log.write(f\"Average F1: {summary['average_token_metrics']['f1']:.4f}\\n\")\n",
    "        final_log.write(\"====================\\n\")\n",
    "\n",
    "        # Additional summary for \"Citation Needed\" class within category 3\n",
    "        final_log.write(\"\\n=== Citation Needed Class within Category 3 Analysis ===\\n\")\n",
    "        final_log.write(f\"Average BERT Similarity: {citation_needed_category_3_summary['average_bert_similarity']['all_sentences']:.4f}\\n\")\n",
    "        final_log.write(f\"Token-Level Precision: {citation_needed_category_3_summary['average_token_metrics']['precision']:.4f}\\n\")\n",
    "        final_log.write(f\"Token-Level Recall: {citation_needed_category_3_summary['average_token_metrics']['recall']:.4f}\\n\")\n",
    "        final_log.write(f\"Token-Level F1: {citation_needed_category_3_summary['average_token_metrics']['f1']:.4f}\\n\")\n",
    "        final_log.write(\"====================\\n\")\n",
    "\n",
    "        # Debugging: Total Metrics\n",
    "        final_log.write(\"\\n=== Debugging Information: Total Metrics ===\\n\")\n",
    "        final_log.write(f\"Total TP (True Positives): {global_tp}\\n\")\n",
    "        final_log.write(f\"Total FP (False Positives): {global_fp}\\n\")\n",
    "        final_log.write(f\"Total FN (False Negatives): {global_fn}\\n\")\n",
    "        final_log.write(\"====================\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "training_data_path = \"training_data.csv\"\n",
    "df = pd.read_csv(training_data_path)\n",
    "\n",
    "# Run the categorization and logging\n",
    "categorize_and_log(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de09ac",
   "metadata": {},
   "source": [
    "### This section does similar action as the previous section, but is used to determine the shuffled result, which is used as the baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5187d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shuffled version, which is serving as the baseline.\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import signal\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from random import shuffle\n",
    "\n",
    "# Timeout handler for long-running LIME explanations\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException\n",
    "\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "# Load SpaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize SentenceTransformer for BERT-based similarity\n",
    "bert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.load_state_dict(torch.load(\"./scibert_fine_tuned_model_LIME.pth\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir='./local_model')\n",
    "\n",
    "# Define prediction function for LIME\n",
    "def predict_proba(texts):\n",
    "    tokens = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# Initialize LIME explainer\n",
    "explainer = LimeTextExplainer(class_names=['No Citation Needed', 'Citation Needed'])\n",
    "\n",
    "# Helper function to clean and tokenize text\n",
    "def clean_and_tokenize(text):\n",
    "    tokens = [token.text for token in nlp(text) if token.text]\n",
    "    return tokens\n",
    "\n",
    "# Generate explanations with cleaned multi-word phrases\n",
    "def generate_explanation(lime_keywords_pos, sentence, label):\n",
    "    doc = nlp(sentence)\n",
    "    multi_word_phrases = [\n",
    "        chunk.text for chunk in doc.noun_chunks\n",
    "    ] + [\n",
    "        entity.text for entity in doc.ents\n",
    "    ] + [\n",
    "        f\"{token.head.text} {token.text}\" for token in doc if token.dep_ == \"amod\"\n",
    "    ]\n",
    "    multi_word_phrases = list(set(multi_word_phrases))\n",
    "    explanation = []\n",
    "    if label == 1:  # Citation Needed\n",
    "        for phrase in multi_word_phrases[:3]:  # Top 3 phrases\n",
    "            explanation.append(f\"The phrase '{phrase}' requires substantiation.\")\n",
    "    else:  # No Citation Needed\n",
    "        explanation.append(\"This sentence provides general background information.\")\n",
    "    return \" \".join(explanation), multi_word_phrases\n",
    "\n",
    "# Main processing function with periodic and final score summaries\n",
    "def process_and_compute_similarity(df, lime_timeout=10):\n",
    "    # Shuffle LLM explanations\n",
    "    shuffled_explanations = df['explanation'].tolist()\n",
    "    shuffle(shuffled_explanations)\n",
    "    df['shuffled_explanation'] = shuffled_explanations\n",
    "\n",
    "    # Lists to store results\n",
    "    bert_scores_original = []\n",
    "    bert_scores_shuffled = []\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing sentences\"):\n",
    "        sentence = row['current_sentence']\n",
    "        llm_explanation = row['explanation']\n",
    "        shuffled_explanation = row['shuffled_explanation']\n",
    "        expected_label = row['label']\n",
    "\n",
    "        # Get predicted probabilities and label\n",
    "        predicted_probs = predict_proba([sentence])[0]\n",
    "        predicted_label = int(predicted_probs.argmax())\n",
    "\n",
    "        # Skip sentences where predicted label does not match expected label\n",
    "        if predicted_label != expected_label:\n",
    "            continue\n",
    "\n",
    "        signal.alarm(lime_timeout)\n",
    "        try:\n",
    "            explanation = explainer.explain_instance(\n",
    "                sentence,\n",
    "                predict_proba,\n",
    "                num_features=10,\n",
    "                num_samples=200\n",
    "            )\n",
    "            signal.alarm(0)\n",
    "\n",
    "            # Retrieve LIME results\n",
    "            lime_results = explanation.as_list()\n",
    "            lime_keywords_pos = [\n",
    "                (word, token.pos_)\n",
    "                for word, _ in lime_results\n",
    "                for token in nlp(str(word))\n",
    "            ]\n",
    "\n",
    "            # Generate explanation\n",
    "            generated_explanation, lime_phrases = generate_explanation(\n",
    "                lime_keywords_pos, sentence, predicted_label\n",
    "            )\n",
    "\n",
    "            # Compute BERT similarity scores\n",
    "            score_original = util.cos_sim(\n",
    "                bert_model.encode(generated_explanation, convert_to_tensor=True),\n",
    "                bert_model.encode(llm_explanation, convert_to_tensor=True)\n",
    "            ).item()\n",
    "            score_shuffled = util.cos_sim(\n",
    "                bert_model.encode(generated_explanation, convert_to_tensor=True),\n",
    "                bert_model.encode(shuffled_explanation, convert_to_tensor=True)\n",
    "            ).item()\n",
    "\n",
    "            # Save scores\n",
    "            bert_scores_original.append(score_original)\n",
    "            bert_scores_shuffled.append(score_shuffled)\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(f\"Skipped sentence due to timeout: {sentence}\")\n",
    "            signal.alarm(0)\n",
    "            continue\n",
    "\n",
    "        # Print averages every 500 sentences\n",
    "        if (i + 1) % 500 == 0:\n",
    "            avg_original = sum(bert_scores_original) / len(bert_scores_original)\n",
    "            avg_shuffled = sum(bert_scores_shuffled) / len(bert_scores_shuffled)\n",
    "            print(f\"\\nProcessed {i + 1} sentences:\")\n",
    "            print(f\" - Average BERT Similarity (Original): {avg_original:.4f}\")\n",
    "            print(f\" - Average BERT Similarity (Shuffled): {avg_shuffled:.4f}\\n\")\n",
    "\n",
    "    # Store similarity scores in the DataFrame\n",
    "    df['bert_similarity_original'] = pd.Series(bert_scores_original)\n",
    "    df['bert_similarity_shuffled'] = pd.Series(bert_scores_shuffled)\n",
    "\n",
    "    # Print final overall averages\n",
    "    final_avg_original = sum(bert_scores_original) / len(bert_scores_original)\n",
    "    final_avg_shuffled = sum(bert_scores_shuffled) / len(bert_scores_shuffled)\n",
    "    print(\"\\n=== Final Summary ===\")\n",
    "    print(f\" - Overall Mean BERT Similarity (Original): {final_avg_original:.4f}\")\n",
    "    print(f\" - Overall Mean BERT Similarity (Shuffled): {final_avg_shuffled:.4f}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load the training sample and run\n",
    "training_data_path = \"training_data.csv\"\n",
    "df = pd.read_csv(training_data_path)\n",
    "\n",
    "processed_df = process_and_compute_similarity(df)\n",
    "processed_df.to_csv(\"processed_with_bert_similarity.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2bd970",
   "metadata": {},
   "source": [
    "### This section is a interactive playground, where the user can input any sentence for classification and explanation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fca4468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_3974515/3007878117.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"./scibert_fine_tuned_model_LIME.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence to classify and explain, or type 'quit' to exit.\n",
      "\n",
      "Enter a sentence: quit\n",
      "Exiting interactive classification. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# The interactive playground\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import signal\n",
    "import string\n",
    "\n",
    "# Timeout handler for long-running LIME explanations\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException\n",
    "\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "# Load SpaCy's English model and stop words\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "punctuation_table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "# Initialize SentenceTransformer for BERT-based similarity\n",
    "bert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.load_state_dict(torch.load(\"./scibert_fine_tuned_model_LIME.pth\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir='./local_model')\n",
    "\n",
    "# Define prediction function for LIME\n",
    "def predict_proba(texts):\n",
    "    tokens = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# Initialize LIME explainer\n",
    "explainer = LimeTextExplainer(class_names=['No Citation Needed', 'Citation Needed'])\n",
    "\n",
    "# Helper function to clean and tokenize text\n",
    "def clean_and_tokenize(text):\n",
    "    text = text.translate(punctuation_table).lower()  # Remove punctuation and lowercase\n",
    "    tokens = [token.text for token in nlp(text) if token.text not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Generate explanations with cleaned multi-word phrases\n",
    "def generate_explanation(lime_keywords_pos, sentence, label):\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Extract multi-word phrases\n",
    "    multi_word_phrases = [\n",
    "        chunk.text for chunk in doc.noun_chunks\n",
    "    ] + [\n",
    "        entity.text for entity in doc.ents\n",
    "    ] + [\n",
    "        f\"{token.head.text} {token.text}\" for token in doc if token.dep_ == \"amod\"\n",
    "    ]\n",
    "\n",
    "    # Remove duplicates, normalize, and filter out empty strings\n",
    "    multi_word_phrases = list(set([\n",
    "        ' '.join(sorted([word for word in phrase.lower().split() if word not in stop_words])).strip()\n",
    "        for phrase in multi_word_phrases\n",
    "    ]))\n",
    "    multi_word_phrases = [phrase for phrase in multi_word_phrases if phrase]  # Remove empty strings\n",
    "\n",
    "    # Extract relationships\n",
    "    relationships = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in {\"nsubj\", \"nsubjpass\"} and token.head.pos_ == \"VERB\":\n",
    "            objs = [obj.text for obj in token.head.children if obj.dep_ in {\"dobj\", \"pobj\", \"attr\"}]\n",
    "            relationships.append((token.text, token.head.text, objs))\n",
    "\n",
    "    explanation = []\n",
    "    global used_phrases\n",
    "    used_phrases = []  # Track used phrases throughout the explanation generation process\n",
    "\n",
    "    if label == 1:  # \"Citation Needed\" template\n",
    "        for subj, verb, objs in relationships:\n",
    "            if objs:\n",
    "                obj_list = \", \".join(objs)\n",
    "                standardized_subj = ' '.join(sorted(subj.lower().split()))\n",
    "                if standardized_subj not in used_phrases:\n",
    "                    explanation_text = f\"The effect of action '{verb.upper()}' on '{subj}' and '{obj_list}' requires evidence to support the claim.\"\n",
    "                    explanation.append(explanation_text)\n",
    "                    used_phrases.append(standardized_subj)\n",
    "        for phrase in multi_word_phrases[:3]:  # Limit to top 3 phrases to reduce redundancy\n",
    "            standardized_phrase = ' '.join(sorted(phrase.lower().split()))\n",
    "            if standardized_phrase not in used_phrases:\n",
    "                explanation_text = f\"The phrase '{phrase}' presents a claim that requires substantiation.\"\n",
    "                explanation.append(explanation_text)\n",
    "                used_phrases.append(standardized_phrase)\n",
    "\n",
    "    else:  # \"No Citation Needed\" template\n",
    "        if \"result\" in sentence.lower() or \"method\" in sentence.lower():\n",
    "            if \"result\" not in used_phrases:\n",
    "                explanation_text = \"This sentence reports study-specific findings, not requiring citation.\"\n",
    "                explanation.append(explanation_text)\n",
    "                used_phrases.append(\"result\")\n",
    "        elif any(token.dep_ == \"ROOT\" and token.pos_ == \"AUX\" for token in doc):\n",
    "            if \"logical deduction\" not in used_phrases:\n",
    "                explanation_text = \"This sentence reflects logical deduction and doesn't need citation.\"\n",
    "                explanation.append(explanation_text)\n",
    "                used_phrases.append(\"logical deduction\")\n",
    "        elif any(token.ent_type_ in {\"DATE\", \"TIME\", \"PERCENT\", \"QUANTITY\"} for token in doc):\n",
    "            if \"quantitative information\" not in used_phrases:\n",
    "                explanation_text = \"This sentence provides quantitative information that is self-contained.\"\n",
    "                explanation.append(explanation_text)\n",
    "                used_phrases.append(\"quantitative information\")\n",
    "        elif any(token.text.lower() in {\"this\", \"these\", \"our\"} for token in doc):\n",
    "            if \"specific study findings\" not in used_phrases:\n",
    "                explanation_text = \"This sentence refers to specific study findings, which are sufficiently explained and don't need external citation.\"\n",
    "                explanation.append(explanation_text)\n",
    "                used_phrases.append(\"specific study findings\")\n",
    "        else:\n",
    "            if \"general background information\" not in used_phrases:\n",
    "                explanation_text = \"This sentence provides general background information, making external citation unnecessary in this context.\"\n",
    "                explanation.append(explanation_text)\n",
    "                used_phrases.append(\"general background information\")\n",
    "\n",
    "    if not explanation:\n",
    "        explanation.append(\"This sentence provides background or context without requiring citation.\")\n",
    "    \n",
    "    return \" \".join(explanation)\n",
    "\n",
    "# Interactive sentence classification and explanation\n",
    "def interactive_classification():\n",
    "    print(\"Enter a sentence to classify and explain, or type 'quit' to exit.\")\n",
    "    while True:\n",
    "        sentence = input(\"\\nEnter a sentence: \").strip()\n",
    "        if sentence.lower() == \"quit\":\n",
    "            print(\"Exiting interactive classification. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # Predict probabilities and determine label\n",
    "            predicted_probs = predict_proba([sentence])[0]\n",
    "            predicted_label = int(predicted_probs.argmax())\n",
    "            label_name = \"Citation Needed\" if predicted_label == 1 else \"No Citation Needed\"\n",
    "\n",
    "            # Generate explanation using LIME\n",
    "            signal.alarm(10)  # Set a timeout for the explanation\n",
    "            try:\n",
    "                explanation = explainer.explain_instance(\n",
    "                    sentence,\n",
    "                    predict_proba,\n",
    "                    num_features=10,\n",
    "                    num_samples=200\n",
    "                )\n",
    "                lime_results = explanation.as_list()\n",
    "                lime_keywords_pos = [\n",
    "                    (word, token.pos_)\n",
    "                    for word, _ in lime_results\n",
    "                    for token in nlp(str(word))\n",
    "                ]\n",
    "                signal.alarm(0)\n",
    "\n",
    "                # Generate explanation text\n",
    "                generated_explanation = generate_explanation(lime_keywords_pos, sentence, predicted_label)\n",
    "            except TimeoutException:\n",
    "                print(\"The explanation generation timed out. Try a shorter or simpler sentence.\")\n",
    "                continue\n",
    "\n",
    "            # Display results\n",
    "            print(f\"\\n=== Classification ===\")\n",
    "            print(f\"Sentence: {sentence}\")\n",
    "            print(f\"Predicted Label: {label_name}\")\n",
    "            print(f\"Generated Explanation: {generated_explanation}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# Run interactive classification\n",
    "interactive_classification()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305efe05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (virtual env)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
