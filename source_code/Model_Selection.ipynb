{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c8e04ea",
   "metadata": {},
   "source": [
    "# YCITE: Selecting the best model and training it with LIME\n",
    "\n",
    "This notebook consist of code that are used to select the best model and retrain it with LIME."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f68313",
   "metadata": {},
   "source": [
    "### The next section runs 5 BERT based model and evaluate their performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b5ad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertForSequenceClassification, AutoTokenizer, AdamW, get_scheduler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', cache_dir='./local_model')\n",
    "\n",
    "# Training parameters\n",
    "lr = 2e-5\n",
    "weight_decay = 0.01\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Load the dataset\n",
    "training_data_path = \"training_data.csv\"\n",
    "df = pd.read_csv(training_data_path)\n",
    "\n",
    "# Handle missing values\n",
    "df['previous_sentence'] = df['previous_sentence'].fillna('N/A')\n",
    "df.dropna(subset=['current_sentence', 'label'], inplace=True)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Split the data into train (80%), validation (10%), and test (10%)\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.111, random_state=42)  # 10% of 90%\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_pair(row):\n",
    "    try:\n",
    "        tokens = tokenizer(\n",
    "            row['previous_sentence'],\n",
    "            row['current_sentence'],\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return tokens['input_ids'].squeeze(0).tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing pair: {e}\")\n",
    "        return []\n",
    "\n",
    "# Tokenize the datasets\n",
    "def prepare_data(df):\n",
    "    input_ids = df.apply(tokenize_pair, axis=1).tolist()\n",
    "    labels = df['label'].tolist()\n",
    "\n",
    "    input_ids_tensor = torch.tensor(input_ids, dtype=torch.long)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(input_ids_tensor, labels_tensor)\n",
    "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "# Create DataLoaders for train, validation, and test datasets\n",
    "train_loader = prepare_data(train_df)\n",
    "val_loader = prepare_data(val_df)\n",
    "test_loader = prepare_data(test_df)\n",
    "\n",
    "# Set up the optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, no_deprecation_warning=True)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training function with checkpoint saving\n",
    "def train():\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        progress_bar = tqdm(train_loader, desc='Training', leave=False)\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            input_ids, labels = [x.to(device) for x in batch]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1} Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint after each epoch\n",
    "        checkpoint_path = f'./bert_checkpoint_epoch_{epoch + 1}.pth'\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved checkpoint: {checkpoint_path}\")\n",
    "\n",
    "        # Evaluate after each epoch\n",
    "        evaluate(val_loader)\n",
    "\n",
    "# Evaluation function for validation and test datasets\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    predictions, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, batch_labels = [x.to(device) for x in batch]\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "            batch_predictions = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "\n",
    "            predictions.extend(batch_predictions)\n",
    "            labels.extend(batch_labels.cpu().tolist())\n",
    "\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Train the model and validate after each epoch\n",
    "train()\n",
    "\n",
    "# Final evaluation on the test dataset\n",
    "print(\"Evaluating on the Test Dataset:\")\n",
    "evaluate(test_loader)\n",
    "\n",
    "# Save the final fine-tuned model\n",
    "torch.save(model.state_dict(), './fine_tuned_bert_model_final.pth')\n",
    "print(\"Fine-tuned model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da5fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scibert\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW, get_scheduler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize SciBERT model and tokenizer\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir='./local_model')\n",
    "model.to(device)\n",
    "\n",
    "# Training parameters\n",
    "lr = 2e-5\n",
    "weight_decay = 0.01\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Load the dataset\n",
    "training_data_path = \"training_data.csv\"\n",
    "df = pd.read_csv(training_data_path)\n",
    "\n",
    "# Handle missing values\n",
    "df['previous_sentence'] = df['previous_sentence'].fillna('N/A')\n",
    "df.dropna(subset=['current_sentence', 'label'], inplace=True)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Split the data into train (80%), validation (10%), and test (10%)\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.111, random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "def tokenize_pair(row):\n",
    "    \"\"\"Tokenize a pair of sentences.\"\"\"\n",
    "    try:\n",
    "        tokens = tokenizer(\n",
    "            row['previous_sentence'],\n",
    "            row['current_sentence'],\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return tokens['input_ids'].squeeze(0).tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing pair: {e}\")\n",
    "        return []\n",
    "\n",
    "def prepare_data(df):\n",
    "    \"\"\"Prepare DataLoader from tokenized data.\"\"\"\n",
    "    input_ids = df.apply(tokenize_pair, axis=1).tolist()\n",
    "    labels = df['label'].tolist()\n",
    "\n",
    "    input_ids_tensor = torch.tensor(input_ids, dtype=torch.long)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(input_ids_tensor, labels_tensor)\n",
    "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "# Create DataLoaders for train, validation, and test datasets\n",
    "train_loader = prepare_data(train_df)\n",
    "val_loader = prepare_data(val_df)\n",
    "test_loader = prepare_data(test_df)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, no_deprecation_warning=True)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "def train():\n",
    "    \"\"\"Train the model and save checkpoints.\"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        progress_bar = tqdm(train_loader, desc='Training', leave=False)\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            input_ids, labels = [x.to(device) for x in batch]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1} Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint after each epoch\n",
    "        checkpoint_path = f'./scibert_checkpoint_epoch_{epoch + 1}.pth'\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved checkpoint: {checkpoint_path}\")\n",
    "\n",
    "        evaluate(val_loader)\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    \"\"\"Evaluate the model on a given dataset.\"\"\"\n",
    "    model.eval()\n",
    "    predictions, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, batch_labels = [x.to(device) for x in batch]\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "            batch_predictions = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "\n",
    "            predictions.extend(batch_predictions)\n",
    "            labels.extend(batch_labels.cpu().tolist())\n",
    "\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Train the model and validate after each epoch\n",
    "train()\n",
    "\n",
    "# Final evaluation on the test dataset\n",
    "print(\"Evaluating on the Test Dataset:\")\n",
    "evaluate(test_loader)\n",
    "\n",
    "# Save the final fine-tuned model\n",
    "final_model_path = './scibert_fine_tuned_model.pth'\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model saved successfully at: {final_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9698e86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distillbert\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW, get_scheduler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize DistilBERT model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir='./local_model')\n",
    "model.to(device)\n",
    "\n",
    "# Training parameters\n",
    "lr = 2e-5\n",
    "weight_decay = 0.01\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Load the dataset\n",
    "training_data_path = \"training_data.csv\"\n",
    "df = pd.read_csv(training_data_path)\n",
    "\n",
    "# Handle missing values\n",
    "df['previous_sentence'] = df['previous_sentence'].fillna('N/A')\n",
    "df.dropna(subset=['current_sentence', 'label'], inplace=True)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Split the data into train (80%), validation (10%), and test (10%)\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.111, random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "def tokenize_pair(row):\n",
    "    \"\"\"Tokenize a pair of sentences.\"\"\"\n",
    "    try:\n",
    "        tokens = tokenizer(\n",
    "            row['previous_sentence'],\n",
    "            row['current_sentence'],\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return tokens['input_ids'].squeeze(0).tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing pair: {e}\")\n",
    "        return []\n",
    "\n",
    "def prepare_data(df):\n",
    "    \"\"\"Prepare DataLoader from tokenized data.\"\"\"\n",
    "    input_ids = df.apply(tokenize_pair, axis=1).tolist()\n",
    "    labels = df['label'].tolist()\n",
    "\n",
    "    input_ids_tensor = torch.tensor(input_ids, dtype=torch.long)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(input_ids_tensor, labels_tensor)\n",
    "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "# Create DataLoaders for train, validation, and test datasets\n",
    "train_loader = prepare_data(train_df)\n",
    "val_loader = prepare_data(val_df)\n",
    "test_loader = prepare_data(test_df)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, no_deprecation_warning=True)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "def train():\n",
    "    \"\"\"Train the model and save checkpoints.\"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        progress_bar = tqdm(train_loader, desc='Training', leave=False)\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            input_ids, labels = [x.to(device) for x in batch]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1} Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint after each epoch\n",
    "        checkpoint_path = f'./distilbert_checkpoint_epoch_{epoch + 1}.pth'\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved checkpoint: {checkpoint_path}\")\n",
    "\n",
    "        evaluate(val_loader)\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    \"\"\"Evaluate the model on a given dataset.\"\"\"\n",
    "    model.eval()\n",
    "    predictions, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, batch_labels = [x.to(device) for x in batch]\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "            batch_predictions = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "\n",
    "            predictions.extend(batch_predictions)\n",
    "            labels.extend(batch_labels.cpu().tolist())\n",
    "\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Train the model and validate after each epoch\n",
    "train()\n",
    "\n",
    "# Final evaluation on the test dataset\n",
    "print(\"Evaluating on the Test Dataset:\")\n",
    "evaluate(test_loader)\n",
    "\n",
    "# Save the final fine-tuned model\n",
    "final_model_path = './distilbert_fine_tuned_model.pth'\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model saved successfully at: {final_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce49f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roberta\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW, get_scheduler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize RoBERTa model and tokenizer\n",
    "model_name = \"roberta-base\"  # You can also use 'roberta-large'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir='./local_model')\n",
    "model.to(device)\n",
    "\n",
    "# Training parameters\n",
    "lr = 2e-5\n",
    "weight_decay = 0.01\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Load the dataset\n",
    "training_data_path = \"training_data.csv\"\n",
    "df = pd.read_csv(training_data_path)\n",
    "\n",
    "# Handle missing values\n",
    "df['previous_sentence'] = df['previous_sentence'].fillna('N/A')\n",
    "df.dropna(subset=['current_sentence', 'label'], inplace=True)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Split the data into train (80%), validation (10%), and test (10%)\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.111, random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "def tokenize_pair(row):\n",
    "    \"\"\"Tokenize a pair of sentences.\"\"\"\n",
    "    try:\n",
    "        tokens = tokenizer(\n",
    "            row['previous_sentence'],\n",
    "            row['current_sentence'],\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return tokens['input_ids'].squeeze(0).tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing pair: {e}\")\n",
    "        return []\n",
    "\n",
    "def prepare_data(df):\n",
    "    \"\"\"Prepare DataLoader from tokenized data.\"\"\"\n",
    "    input_ids = df.apply(tokenize_pair, axis=1).tolist()\n",
    "    labels = df['label'].tolist()\n",
    "\n",
    "    input_ids_tensor = torch.tensor(input_ids, dtype=torch.long)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(input_ids_tensor, labels_tensor)\n",
    "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "# Create DataLoaders for train, validation, and test datasets\n",
    "train_loader = prepare_data(train_df)\n",
    "val_loader = prepare_data(val_df)\n",
    "test_loader = prepare_data(test_df)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, no_deprecation_warning=True)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "def train():\n",
    "    \"\"\"Train the model and save checkpoints.\"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        progress_bar = tqdm(train_loader, desc='Training', leave=False)\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            input_ids, labels = [x.to(device) for x in batch]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1} Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint after each epoch\n",
    "        checkpoint_path = f'./roberta_checkpoint_epoch_{epoch + 1}.pth'\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved checkpoint: {checkpoint_path}\")\n",
    "\n",
    "        evaluate(val_loader)\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    \"\"\"Evaluate the model on a given dataset.\"\"\"\n",
    "    model.eval()\n",
    "    predictions, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, batch_labels = [x.to(device) for x in batch]\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "            batch_predictions = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "\n",
    "            predictions.extend(batch_predictions)\n",
    "            labels.extend(batch_labels.cpu().tolist())\n",
    "\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Train the model and validate after each epoch\n",
    "train()\n",
    "\n",
    "# Final evaluation on the test dataset\n",
    "print(\"Evaluating on the Test Dataset:\")\n",
    "evaluate(test_loader)\n",
    "\n",
    "# Save the final fine-tuned model\n",
    "final_model_path = './roberta_fine_tuned_model.pth'\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model saved successfully at: {final_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# biobert\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW, get_scheduler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize BioBERT model and tokenizer\n",
    "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir='./local_model')\n",
    "model.to(device)\n",
    "\n",
    "# Training parameters\n",
    "lr = 2e-5\n",
    "weight_decay = 0.01\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Load the dataset\n",
    "training_data_path = \"training_data.csv\"\n",
    "df = pd.read_csv(training_data_path)\n",
    "\n",
    "# Handle missing values\n",
    "df['previous_sentence'] = df['previous_sentence'].fillna('N/A')\n",
    "df.dropna(subset=['current_sentence', 'label'], inplace=True)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Split the data into train (80%), validation (10%), and test (10%)\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.111, random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "def tokenize_pair(row):\n",
    "    \"\"\"Tokenize a pair of sentences.\"\"\"\n",
    "    try:\n",
    "        tokens = tokenizer(\n",
    "            row['previous_sentence'],\n",
    "            row['current_sentence'],\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return tokens['input_ids'].squeeze(0).tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing pair: {e}\")\n",
    "        return []\n",
    "\n",
    "def prepare_data(df):\n",
    "    \"\"\"Prepare DataLoader from tokenized data.\"\"\"\n",
    "    input_ids = df.apply(tokenize_pair, axis=1).tolist()\n",
    "    labels = df['label'].tolist()\n",
    "\n",
    "    input_ids_tensor = torch.tensor(input_ids, dtype=torch.long)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(input_ids_tensor, labels_tensor)\n",
    "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "# Create DataLoaders for train, validation, and test datasets\n",
    "train_loader = prepare_data(train_df)\n",
    "val_loader = prepare_data(val_df)\n",
    "test_loader = prepare_data(test_df)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, no_deprecation_warning=True)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "def train():\n",
    "    \"\"\"Train the model and save checkpoints.\"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        progress_bar = tqdm(train_loader, desc='Training', leave=False)\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            input_ids, labels = [x.to(device) for x in batch]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1} Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint after each epoch\n",
    "        checkpoint_path = f'./biobert_checkpoint_epoch_{epoch + 1}.pth'\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved checkpoint: {checkpoint_path}\")\n",
    "\n",
    "        evaluate(val_loader)\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    \"\"\"Evaluate the model on a given dataset.\"\"\"\n",
    "    model.eval()\n",
    "    predictions, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, batch_labels = [x.to(device) for x in batch]\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "            batch_predictions = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "\n",
    "            predictions.extend(batch_predictions)\n",
    "            labels.extend(batch_labels.cpu().tolist())\n",
    "\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Train the model and validate after each epoch\n",
    "train()\n",
    "\n",
    "# Final evaluation on the test dataset\n",
    "print(\"Evaluating on the Test Dataset:\")\n",
    "evaluate(test_loader)\n",
    "\n",
    "# Save the final fine-tuned model\n",
    "final_model_path = './biobert_fine_tuned_model.pth'\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model saved successfully at: {final_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f00035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeBerta\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW, get_scheduler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize DeBERTa model and tokenizer\n",
    "model_name = \"microsoft/deberta-v3-base\"  # You can use 'microsoft/deberta-v2-xlarge' if needed\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir='./local_model')\n",
    "model.to(device)\n",
    "\n",
    "# Training parameters\n",
    "lr = 2e-5\n",
    "weight_decay = 0.01\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Load the dataset\n",
    "training_data_path = \"training_data.csv\"\n",
    "df = pd.read_csv(training_data_path)\n",
    "\n",
    "# Handle missing values\n",
    "df['previous_sentence'] = df['previous_sentence'].fillna('N/A')\n",
    "df.dropna(subset=['current_sentence', 'label'], inplace=True)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Split the data into train (80%), validation (10%), and test (10%)\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.111, random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "def tokenize_pair(row):\n",
    "    \"\"\"Tokenize a pair of sentences.\"\"\"\n",
    "    try:\n",
    "        tokens = tokenizer(\n",
    "            row['previous_sentence'],\n",
    "            row['current_sentence'],\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return tokens['input_ids'].squeeze(0).tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing pair: {e}\")\n",
    "        return []\n",
    "\n",
    "def prepare_data(df):\n",
    "    \"\"\"Prepare DataLoader from tokenized data.\"\"\"\n",
    "    input_ids = df.apply(tokenize_pair, axis=1).tolist()\n",
    "    labels = df['label'].tolist()\n",
    "\n",
    "    input_ids_tensor = torch.tensor(input_ids, dtype=torch.long)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(input_ids_tensor, labels_tensor)\n",
    "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "# Create DataLoaders for train, validation, and test datasets\n",
    "train_loader = prepare_data(train_df)\n",
    "val_loader = prepare_data(val_df)\n",
    "test_loader = prepare_data(test_df)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, no_deprecation_warning=True)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "def train():\n",
    "    \"\"\"Train the model and save checkpoints.\"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        progress_bar = tqdm(train_loader, desc='Training', leave=False)\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            input_ids, labels = [x.to(device) for x in batch]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1} Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint after each epoch\n",
    "        checkpoint_path = f'./deberta_checkpoint_epoch_{epoch + 1}.pth'\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved checkpoint: {checkpoint_path}\")\n",
    "\n",
    "        evaluate(val_loader)\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    \"\"\"Evaluate the model on a given dataset.\"\"\"\n",
    "    model.eval()\n",
    "    predictions, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, batch_labels = [x.to(device) for x in batch]\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "            batch_predictions = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "\n",
    "            predictions.extend(batch_predictions)\n",
    "            labels.extend(batch_labels.cpu().tolist())\n",
    "\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Train the model and validate after each epoch\n",
    "train()\n",
    "\n",
    "# Final evaluation on the test dataset\n",
    "print(\"Evaluating on the Test Dataset:\")\n",
    "evaluate(test_loader)\n",
    "\n",
    "# Save the final fine-tuned model\n",
    "final_model_path = './deberta_fine_tuned_model.pth'\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model saved successfully at: {final_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df987334",
   "metadata": {},
   "source": [
    "### After selecting SciBERT as the model, the next section retrains the model (SciBERT) with LIME. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f24fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW, get_scheduler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import numpy as np\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize SciBERT model and tokenizer\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir='./local_model')\n",
    "model.to(device)\n",
    "\n",
    "# Training parameters\n",
    "lr = 2e-5\n",
    "weight_decay = 0.01\n",
    "EPOCHS = 4\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Load the dataset\n",
    "training_data_path = \"training_data.csv\"\n",
    "df = pd.read_csv(training_data_path)\n",
    "\n",
    "# Handle missing values\n",
    "df['previous_sentence'] = df['previous_sentence'].fillna('N/A')\n",
    "df.dropna(subset=['current_sentence', 'label'], inplace=True)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Split the data into train (80%), validation (10%), and test (10%)\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.111, random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "def tokenize_pair(row):\n",
    "    \"\"\"Tokenize a pair of sentences.\"\"\"\n",
    "    try:\n",
    "        tokens = tokenizer(\n",
    "            row['previous_sentence'],\n",
    "            row['current_sentence'],\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return tokens['input_ids'].squeeze(0).tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing pair: {e}\")\n",
    "        return []\n",
    "\n",
    "def prepare_data(df):\n",
    "    \"\"\"Prepare DataLoader from tokenized data.\"\"\"\n",
    "    input_ids = df.apply(tokenize_pair, axis=1).tolist()\n",
    "    labels = df['label'].tolist()\n",
    "\n",
    "    input_ids_tensor = torch.tensor(input_ids, dtype=torch.long)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(input_ids_tensor, labels_tensor)\n",
    "    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "# Create DataLoaders for train, validation, and test datasets\n",
    "train_loader = prepare_data(train_df)\n",
    "val_loader = prepare_data(val_df)\n",
    "test_loader = prepare_data(test_df)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "def train():\n",
    "    \"\"\"Train the model and save checkpoints.\"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        progress_bar = tqdm(train_loader, desc='Training', leave=False)\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            input_ids, labels = [x.to(device) for x in batch]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1} Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint after each epoch\n",
    "        checkpoint_path = f'./scibert_LIME_checkpoint_epoch_{epoch + 1}.pth'\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved checkpoint: {checkpoint_path}\")\n",
    "\n",
    "        evaluate(val_loader)\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    \"\"\"Evaluate the model on a given dataset.\"\"\"\n",
    "    model.eval()\n",
    "    predictions, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, batch_labels = [x.to(device) for x in batch]\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "            batch_predictions = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "\n",
    "            predictions.extend(batch_predictions)\n",
    "            labels.extend(batch_labels.cpu().tolist())\n",
    "\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Train the model and validate after each epoch\n",
    "train()\n",
    "\n",
    "# Final evaluation on the test dataset\n",
    "print(\"Evaluating on the Test Dataset:\")\n",
    "evaluate(test_loader)\n",
    "\n",
    "# Save the final fine-tuned model\n",
    "final_model_path = './scibert_fine_tuned_model_LIME.pth'\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model saved successfully at: {final_model_path}\")\n",
    "\n",
    "# LIME Integration\n",
    "def predict_proba(texts):\n",
    "    \"\"\"Get model probabilities for given texts.\"\"\"\n",
    "    model.eval()\n",
    "    tokens = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# Initialize LIME explainer\n",
    "explainer = LimeTextExplainer(class_names=['No Citation Needed', 'Citation Needed'])\n",
    "\n",
    "# Sample a sentence for explanation\n",
    "sample_text = test_df['current_sentence'].iloc[0]  # Choose any test sentence\n",
    "\n",
    "# Generate explanation\n",
    "explanation = explainer.explain_instance(\n",
    "    sample_text,  # The text instance to explain\n",
    "    predict_proba,  # Prediction function\n",
    "    num_features=10,  # Number of words to highlight in explanation\n",
    "    num_samples=500  # Number of perturbations to generate\n",
    ")\n",
    "\n",
    "# Display explanation\n",
    "explanation.show_in_notebook(text=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
